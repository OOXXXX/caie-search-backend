#!/usr/bin/env python3
"""
æœç´¢æœåŠ¡
åŸºäºElasticsearchçš„æ™ºèƒ½æœç´¢å¼•æ“
"""

import json
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path
import logging

from elasticsearch import Elasticsearch, AsyncElasticsearch
from elasticsearch.exceptions import NotFoundError, RequestError
import numpy as np
from sentence_transformers import SentenceTransformer

from models import SearchResult, QuestionData, IndexStats
from caie_math_processor import CAIEMathProcessor
from math_formula_processor import MathFormulaProcessor

class SearchService:
    def __init__(self, elasticsearch_url: str):
        """åˆå§‹åŒ–æœç´¢æœåŠ¡"""
        self.logger = logging.getLogger(__name__)
        self.es_url = elasticsearch_url
        
        # åŒæ­¥å’Œå¼‚æ­¥Elasticsearchå®¢æˆ·ç«¯
        self.es = Elasticsearch([elasticsearch_url])
        self.async_es = AsyncElasticsearch([elasticsearch_url])
        
        self.index_name = "caie_math_questions"
        
        # åˆå§‹åŒ–æ•°å­¦å…¬å¼å¤„ç†å™¨
        self.math_processor = MathFormulaProcessor()
        
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰
        try:
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            self.logger.info("âœ… å‘é‡æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"âš ï¸  å‘é‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.embedding_model = None
    
    async def initialize(self):
        """åˆå§‹åŒ–æœç´¢æœåŠ¡"""
        try:
            # æ£€æŸ¥è¿æ¥
            if not self.es.ping():
                raise ConnectionError("æ— æ³•è¿æ¥åˆ°Elasticsearch")
            
            # åˆ›å»ºç´¢å¼•
            await self.create_index()
            
            self.logger.info("âœ… æœç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def create_index(self):
        """åˆ›å»ºElasticsearchç´¢å¼•"""
        index_mapping = {
            "mappings": {
                "properties": {
                    "question_id": {"type": "keyword"},
                    "content": {
                        "type": "text",
                        "analyzer": "standard",
                        "search_analyzer": "standard",
                        "fields": {
                            "keyword": {"type": "keyword"},
                            "english": {
                                "type": "text",
                                "analyzer": "english"
                            }
                        }
                    },
                    "title": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "year": {"type": "keyword"},
                    "season": {"type": "keyword"},
                    "paper_code": {"type": "keyword"},
                    "subject_code": {"type": "keyword"},
                    "mark_scheme": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "file_path": {"type": "keyword"},
                    "embedding": {
                        "type": "dense_vector",
                        "dims": 384  # all-MiniLM-L6-v2çš„å‘é‡ç»´åº¦
                    },
                    "created_at": {"type": "date"}
                }
            },
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "math_analyzer": {
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "stop",
                                "math_synonyms"
                            ]
                        }
                    },
                    "filter": {
                        "math_synonyms": {
                            "type": "synonym",
                            "synonyms": [
                                "differentiate,derivative,diff",
                                "integrate,integration,integral",
                                "solve,find,calculate,determine",
                                "function,equation,formula",
                                "graph,plot,curve,line"
                            ]
                        }
                    }
                }
            }
        }
        
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if self.es.indices.exists(index=self.index_name):
                self.logger.info(f"ç´¢å¼• {self.index_name} å·²å­˜åœ¨")
                return
            
            # åˆ›å»ºç´¢å¼•
            self.es.indices.create(
                index=self.index_name,
                body=index_mapping
            )
            self.logger.info(f"âœ… åˆ›å»ºç´¢å¼• {self.index_name} æˆåŠŸ")
            
        except RequestError as e:
            if "already exists" in str(e):
                self.logger.info(f"ç´¢å¼• {self.index_name} å·²å­˜åœ¨")
            else:
                self.logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
                raise
    
    async def build_index(self):
        """æ„å»ºæœç´¢ç´¢å¼•"""
        self.logger.info("ğŸ”¨ å¼€å§‹æ„å»ºæœç´¢ç´¢å¼•...")
        
        try:
            # ä½¿ç”¨CAIEæ•°å­¦å¤„ç†å™¨æå–æ•°æ®
            processor = CAIEMathProcessor("/Users/patrick/Desktop/Container")
            
            # æ‰«æè¯•å·
            papers = processor.scan_papers()
            self.logger.info(f"æ‰¾åˆ° {len(papers)} ä¸ªè¯•å·æ–‡ä»¶")
            
            # åŒ¹é…é¢˜ç›®å’Œç­”æ¡ˆ
            processor.match_questions_with_answers()
            questions = processor.questions
            
            self.logger.info(f"æå–åˆ° {len(questions)} ä¸ªé¢˜ç›®")
            
            # æ‰¹é‡ç´¢å¼•
            await self._bulk_index_questions(questions)
            
            self.logger.info("âœ… æœç´¢ç´¢å¼•æ„å»ºå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {e}")
            raise
    
    async def _bulk_index_questions(self, questions: List, batch_size: int = 100):
        """æ‰¹é‡ç´¢å¼•é¢˜ç›®"""
        for i in range(0, len(questions), batch_size):
            batch = questions[i:i + batch_size]
            actions = []
            
            for question in batch:
                # ç”Ÿæˆæ–‡æ¡£ID
                doc_id = question.question_id
                
                # å¢å¼ºæ•°å­¦å†…å®¹å¤„ç†
                enhanced_content = self.math_processor.process_pdf_text(question.content)
                math_features = self.math_processor.extract_formula_features(question.content)
                formula_tokens = self.math_processor.tokenize_formula(question.content)
                
                # å‡†å¤‡æ–‡æ¡£æ•°æ®
                doc = {
                    "question_id": question.question_id,
                    "content": enhanced_content,
                    "math_features": " ".join(math_features),
                    "formula_tokens": formula_tokens,
                    "title": f"Question {question.question_id}",
                    "year": question.paper_info.year,
                    "season": question.paper_info.season,
                    "paper_code": question.paper_info.paper_code,
                    "subject_code": "9709",
                    "mark_scheme": question.mark_scheme or "",
                    "file_path": question.paper_info.file_path,
                    "created_at": "2024-01-01T00:00:00"
                }
                
                # ç”Ÿæˆå‘é‡åµŒå…¥
                if self.embedding_model:
                    try:
                        embedding = self.embedding_model.encode(question.content)
                        doc["embedding"] = embedding.tolist()
                    except Exception as e:
                        self.logger.warning(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
                
                # æ·»åŠ åˆ°æ‰¹æ¬¡
                actions.append({
                    "_index": self.index_name,
                    "_id": doc_id,
                    "_source": doc
                })
            
            # æ‰§è¡Œæ‰¹é‡ç´¢å¼• - ä¿®å¤æ ¼å¼
            try:
                # æ„å»ºæ­£ç¡®çš„bulkæ ¼å¼
                bulk_data = []
                for action in actions:
                    bulk_data.append({"index": {"_index": action["_index"], "_id": action["_id"]}})
                    bulk_data.append(action["_source"])
                
                response = self.es.bulk(body=bulk_data)
                if response.get("errors"):
                    self.logger.warning(f"æ‰¹é‡ç´¢å¼•æœ‰é”™è¯¯: {response}")
                else:
                    self.logger.info(f"æˆåŠŸç´¢å¼• {len(actions)} ä¸ªæ–‡æ¡£")
            except Exception as e:
                self.logger.error(f"æ‰¹é‡ç´¢å¼•å¤±è´¥: {e}")
    
    async def search_by_text(
        self, 
        query: str, 
        limit: int = 10,
        filters: Optional[Dict] = None
    ) -> List[SearchResult]:
        """æ–‡æœ¬æœç´¢ - æ”¯æŒæ•°å­¦å…¬å¼å¢å¼º"""
        try:
            # ä½¿ç”¨æ•°å­¦å…¬å¼å¤„ç†å™¨å¢å¼ºæŸ¥è¯¢
            enhanced_queries = self.math_processor.enhance_search_query(query)
            
            # æ„å»ºå¢å¼ºçš„æœç´¢æŸ¥è¯¢
            search_body = {
                "query": {
                    "bool": {
                        "should": [
                            # æ•°å­¦å…¬å¼tokenç²¾ç¡®åŒ¹é… - æœ€é«˜æƒé‡
                            {
                                "terms": {
                                    "formula_tokens": self.math_processor.tokenize_formula(query),
                                    "boost": 5.0
                                }
                            },
                            # æ•°å­¦ç‰¹å¾åŒ¹é…
                            {
                                "multi_match": {
                                    "query": " ".join(self.math_processor.extract_formula_features(query)),
                                    "fields": ["math_features^3"],
                                    "type": "best_fields",
                                    "boost": 4.0
                                }
                            },
                            # æ•°å­¦ç¬¦å·å­—æ®µç²¾ç¡®åŒ¹é…
                            {
                                "match": {
                                    "content.math_symbols": {
                                        "query": enhanced_queries.get('normalized', query),
                                        "boost": 3.5
                                    }
                                }
                            },
                            # æ•°å­¦æ¦‚å¿µåŒ¹é…
                            {
                                "match": {
                                    "content.math_concepts": {
                                        "query": enhanced_queries.get('expanded', query),
                                        "boost": 3.0
                                    }
                                }
                            },
                            # åŸå§‹æŸ¥è¯¢ - ç²¾ç¡®çŸ­è¯­åŒ¹é…
                            {
                                "match_phrase": {
                                    "content": {
                                        "query": enhanced_queries.get('original', query),
                                        "boost": 2.5
                                    }
                                }
                            },
                            # æ ‡å‡†åŒ–æŸ¥è¯¢ - æ•°å­¦ç¬¦å·å¤„ç†
                            {
                                "multi_match": {
                                    "query": enhanced_queries.get('normalized', query),
                                    "fields": ["content^2", "title", "mark_scheme"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO",
                                    "boost": 2.0
                                }
                            },
                            # æ‰©å±•æŸ¥è¯¢ - æ¦‚å¿µåŒä¹‰è¯
                            {
                                "multi_match": {
                                    "query": enhanced_queries.get('expanded', query),
                                    "fields": ["content^1.5", "title", "mark_scheme"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO",
                                    "boost": 1.5
                                }
                            },
                            # æ¨¡ç³ŠåŒ¹é… - å…œåº•æœç´¢
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["content", "title", "mark_scheme"],
                                    "type": "best_fields",
                                    "fuzziness": "AUTO",
                                    "boost": 1.0
                                }
                            },
                        ],
                        "minimum_should_match": 1
                    }
                },
                "size": limit,
                "highlight": {
                    "fields": {
                        "content": {"fragment_size": 200},
                        "math_features": {"fragment_size": 100}
                    }
                }
            }
            
            # æ·»åŠ å‘é‡æœç´¢ - æé«˜æƒé‡ç”¨äºæ•°å­¦å…¬å¼è¯­ä¹‰åŒ¹é…
            if self.embedding_model:
                query_embedding = self.embedding_model.encode(query).tolist()
                search_body["query"]["bool"]["should"].append({
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                            "params": {"query_vector": query_embedding}
                        },
                        "boost": 3.5  # æé«˜å‘é‡æœç´¢æƒé‡
                    }
                })
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            if filters:
                filter_clauses = []
                for field, value in filters.items():
                    filter_clauses.append({"term": {field: value}})
                
                if filter_clauses:
                    search_body["query"]["bool"]["filter"] = filter_clauses
            
            # æ‰§è¡Œæœç´¢
            response = self.es.search(
                index=self.index_name,
                body=search_body
            )
            
            # è§£æç»“æœ
            results = []
            for hit in response["hits"]["hits"]:
                source = hit["_source"]
                
                # è·å–é«˜äº®æ–‡æœ¬
                highlight = hit.get("highlight", {})
                content = highlight.get("content", [source["content"]])[0]
                
                result = SearchResult(
                    id=source["question_id"],
                    title=source["title"],
                    content=content,
                    year=source["year"],
                    season=source["season"],
                    paper_code=source["paper_code"],
                    mark_scheme=source.get("mark_scheme"),
                    confidence=hit["_score"] / 10.0,  # å½’ä¸€åŒ–åˆ†æ•°
                    file_path=source.get("file_path")
                )
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬æœç´¢å¤±è´¥: {e}")
            return []
    
    async def search_by_image_similarity(
        self, 
        image_embedding: List[float], 
        limit: int = 5
    ) -> List[SearchResult]:
        """åŸºäºå›¾åƒå‘é‡çš„ç›¸ä¼¼åº¦æœç´¢"""
        try:
            search_body = {
                "query": {
                    "script_score": {
                        "query": {"match_all": {}},
                        "script": {
                            "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                            "params": {"query_vector": image_embedding}
                        }
                    }
                },
                "size": limit
            }
            
            response = self.es.search(
                index=self.index_name,
                body=search_body
            )
            
            results = []
            for hit in response["hits"]["hits"]:
                source = hit["_source"]
                result = SearchResult(
                    id=source["question_id"],
                    title=source["title"],
                    content=source["content"],
                    year=source["year"],
                    season=source["season"],
                    paper_code=source["paper_code"],
                    mark_scheme=source.get("mark_scheme"),
                    confidence=hit["_score"] / 10.0,
                    file_path=source.get("file_path")
                )
                results.append(result)
            
            return results
            
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    async def get_index_stats(self) -> IndexStats:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.es.indices.stats(index=self.index_name)
            count = self.es.count(index=self.index_name)
            
            return IndexStats(
                total_documents=count["count"],
                index_size=f"{stats['indices'][self.index_name]['total']['store']['size_in_bytes'] / 1024 / 1024:.2f} MB"
            )
            
        except Exception as e:
            self.logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return IndexStats(total_documents=0, index_size="0 MB")
    
    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.async_es.close()
        self.es.close()