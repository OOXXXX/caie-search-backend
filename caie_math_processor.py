#!/usr/bin/env python3
"""
CAIE A-Level Mathematics Paper Processing System
å¤„ç†2001-2022å¹´æ‰€æœ‰CAIEæ•°å­¦è¯•å·ï¼Œæå–é¢˜ç›®å’Œç­”æ¡ˆ
"""

import os
import re
import json
import PyPDF2
# import fitz  # PyMuPDF - æš‚æ—¶æ³¨é‡Šï¼Œä½¿ç”¨PyPDF2æ›¿ä»£
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional
import pandas as pd

@dataclass
class PaperInfo:
    """è¯•å·ä¿¡æ¯ç»“æ„"""
    file_path: str
    year: str
    season: str
    paper_type: str  # 'qp' or 'ms'
    paper_code: str  # e.g., '11', '12', '22'
    subject_code: str = "9709"
    
@dataclass
class Question:
    """é¢˜ç›®ä¿¡æ¯ç»“æ„"""
    question_id: str
    content: str
    image_path: Optional[str] = None
    paper_info: Optional[PaperInfo] = None
    mark_scheme: Optional[str] = None

class CAIEMathProcessor:
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.papers: List[PaperInfo] = []
        self.questions: List[Question] = []
        
    def scan_papers(self) -> List[PaperInfo]:
        """æ‰«æ2020å¹´CAIEæ•°å­¦è¯•å·æ–‡ä»¶ï¼ˆæµ‹è¯•ç‰ˆï¼‰"""
        print("ğŸ” æ‰«æCAIEæ•°å­¦è¯•å·æ–‡ä»¶ï¼ˆä»…2020å¹´æµ‹è¯•æ•°æ®ï¼‰...")
        
        math_path = self.base_path / "CAIE" / "Alevel" / "Mathematics (9709)"
        
        for year_dir in sorted(math_path.iterdir()):
            if not year_dir.is_dir():
                continue
                
            year = year_dir.name
            # åªå¤„ç†2020å¹´æ•°æ®
            if year != "2020":
                continue
                
            print(f"ğŸ“… å¤„ç†å¹´ä»½: {year} (æµ‹è¯•æ¨¡å¼)")
            
            for season_dir in year_dir.iterdir():
                if not season_dir.is_dir():
                    continue
                    
                season = season_dir.name.lower()
                
                # æ‰«æQuestion Paper
                qp_dir = season_dir / "Question_Paper" if (season_dir / "Question_Paper").exists() else season_dir / "Question Paper"
                if qp_dir.exists():
                    for pdf_file in qp_dir.glob("*.pdf"):
                        paper_info = self._parse_filename(str(pdf_file), year, season, "qp")
                        if paper_info:
                            self.papers.append(paper_info)
                
                # æ‰«æMark Scheme
                ms_dir = season_dir / "Mark_Scheme" if (season_dir / "Mark_Scheme").exists() else season_dir / "Mark Scheme"
                if ms_dir.exists():
                    for pdf_file in ms_dir.glob("*.pdf"):
                        paper_info = self._parse_filename(str(pdf_file), year, season, "ms")
                        if paper_info:
                            self.papers.append(paper_info)
        
        print(f"âœ… æ€»å…±æ‰¾åˆ° {len(self.papers)} ä¸ªè¯•å·æ–‡ä»¶")
        return self.papers
    
    def _parse_filename(self, file_path: str, year: str, season: str, paper_type: str) -> Optional[PaperInfo]:
        """è§£ææ–‡ä»¶åæå–è¯•å·ä¿¡æ¯"""
        filename = Path(file_path).name
        
        # åŒ¹é…æ–‡ä»¶åæ¨¡å¼: 9709_s22_qp_12.pdf
        pattern = r"9709_([smw])(\d+)_(qp|ms)_(\w+)\.pdf"
        match = re.search(pattern, filename)
        
        if match:
            season_code, year_code, type_code, paper_code = match.groups()
            return PaperInfo(
                file_path=file_path,
                year=year,
                season=season,
                paper_type=type_code,
                paper_code=paper_code
            )
        
        return None
    
    def extract_questions_from_pdf(self, paper_info: PaperInfo) -> List[Question]:
        """ä»PDFä¸­æå–é¢˜ç›® (ä½¿ç”¨PyPDF2æ›¿ä»£PyMuPDF)"""
        print(f"ğŸ“– å¤„ç†æ–‡ä»¶: {Path(paper_info.file_path).name}")
        
        try:
            with open(paper_info.file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                questions = []
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    
                    # æ£€æµ‹é¢˜ç›®ç¼–å· (1., 2., 3., ç­‰)
                    question_patterns = [
                        r"^\s*(\d+)\.\s*(.+)",  # 1. é¢˜ç›®å†…å®¹
                        r"^\s*(\d+)\s+(.+)",    # 1 é¢˜ç›®å†…å®¹
                    ]
                    
                    for pattern in question_patterns:
                        matches = re.findall(pattern, text, re.MULTILINE | re.DOTALL)
                        for match in matches:
                            question_num, content = match
                            
                            # æ¸…ç†å†…å®¹
                            content = content.strip()
                            if len(content) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                                question = Question(
                                    question_id=f"{paper_info.year}_{paper_info.season}_{paper_info.paper_code}_{question_num}",
                                    content=content[:500],  # é™åˆ¶é•¿åº¦
                                    paper_info=paper_info
                                )
                                questions.append(question)
                
                return questions
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {paper_info.file_path}: {e}")
            return []
    
    def match_questions_with_answers(self):
        """åŒ¹é…é¢˜ç›®ä¸ç­”æ¡ˆ"""
        print("ğŸ”— åŒ¹é…é¢˜ç›®ä¸Mark Scheme...")
        
        # æŒ‰è¯•å·åˆ†ç»„
        qp_papers = [p for p in self.papers if p.paper_type == "qp"]
        ms_papers = [p for p in self.papers if p.paper_type == "ms"]
        
        # ä¸ºæ¯ä¸ªQuestion Paperæ‰¾åˆ°å¯¹åº”çš„Mark Scheme
        for qp in qp_papers:
            # æŸ¥æ‰¾å¯¹åº”çš„Mark Scheme
            matching_ms = [ms for ms in ms_papers 
                          if ms.year == qp.year 
                          and ms.season == qp.season 
                          and ms.paper_code == qp.paper_code]
            
            if matching_ms:
                print(f"âœ… æ‰¾åˆ°åŒ¹é…: {Path(qp.file_path).name} <-> {Path(matching_ms[0].file_path).name}")
                
                # æå–é¢˜ç›®
                questions = self.extract_questions_from_pdf(qp)
                
                # æå–å¯¹åº”ç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼‰
                for question in questions:
                    question.mark_scheme = f"Mark Scheme: {matching_ms[0].file_path}"
                
                self.questions.extend(questions)
    
    def export_to_json(self, output_file: str):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        print(f"ğŸ’¾ å¯¼å‡ºæ•°æ®åˆ° {output_file}")
        
        data = []
        for question in self.questions:
            data.append({
                "id": question.question_id,
                "content": question.content,
                "year": question.paper_info.year,
                "season": question.paper_info.season,
                "paper_code": question.paper_info.paper_code,
                "mark_scheme": question.mark_scheme,
                "file_path": question.paper_info.file_path
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸå¯¼å‡º {len(data)} ä¸ªé¢˜ç›®")
    
    def create_elasticsearch_index(self):
        """åˆ›å»ºElasticsearchç´¢å¼•ç»“æ„"""
        index_mapping = {
            "mappings": {
                "properties": {
                    "question_id": {"type": "keyword"},
                    "content": {
                        "type": "text",
                        "analyzer": "standard",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "year": {"type": "keyword"},
                    "season": {"type": "keyword"},
                    "paper_code": {"type": "keyword"},
                    "subject": {"type": "keyword"},
                    "mark_scheme": {"type": "text"},
                    "file_path": {"type": "keyword"}
                }
            },
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "math_analyzer": {
                            "tokenizer": "standard",
                            "filter": ["lowercase", "stop"]
                        }
                    }
                }
            }
        }
        
        with open("elasticsearch_mapping.json", 'w') as f:
            json.dump(index_mapping, f, indent=2)
        
        print("âœ… Elasticsearchæ˜ å°„é…ç½®å·²ç”Ÿæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ CAIE A-Levelæ•°å­¦è¯•å·å¤„ç†å™¨å¯åŠ¨")
    
    # è®¾ç½®åŸºç¡€è·¯å¾„
    base_path = "/Users/patrick/Desktop/Container"
    processor = CAIEMathProcessor(base_path)
    
    # æ‰«ææ‰€æœ‰è¯•å·
    papers = processor.scan_papers()
    
    # ç»Ÿè®¡ä¿¡æ¯
    qp_count = len([p for p in papers if p.paper_type == "qp"])
    ms_count = len([p for p in papers if p.paper_type == "ms"])
    
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   Question Papers: {qp_count}")
    print(f"   Mark Schemes: {ms_count}")
    print(f"   å¹´ä»½èŒƒå›´: {min(p.year for p in papers)} - {max(p.year for p in papers)}")
    
    # åŒ¹é…é¢˜ç›®å’Œç­”æ¡ˆ
    processor.match_questions_with_answers()
    
    # å¯¼å‡ºæ•°æ®
    processor.export_to_json("caie_math_questions.json")
    
    # ç”ŸæˆElasticsearché…ç½®
    processor.create_elasticsearch_index()
    
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()